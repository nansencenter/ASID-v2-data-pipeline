{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a8932e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c2a985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Alissa\\ENSG\\ING2\\STAGE\\Dev\\satellite-data-pipeline\\asip_v2\n"
     ]
    }
   ],
   "source": [
    "%cd C:/Alissa/ENSG/ING2/STAGE/Dev/satellite-data-pipeline/asip_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc12e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "from os.path import basename, dirname, join\n",
    "import autokeras as ak\n",
    "import ipdb\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "\n",
    "\n",
    "from archive import Archive\n",
    "from data_generator import DataGeneratorFrom_npz_File\n",
    "from utility import Configure, create_model, between_zero_and_one_float_type\n",
    "from train_model import FileBasedConfigure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43c6c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = Archive\n",
    "#print(dir(arch))\n",
    "arch.batch_size = 5\n",
    "arch.beginning_day_of_year =  1\n",
    "arch.ending_day_of_year =  365\n",
    "arch.shuffle_on_epoch_end = True\n",
    "arch.shuffle_for_training = True\n",
    "arch.percentage_of_training = 0.8\n",
    "arch.DATAPATH = 'D:/training/output'\n",
    "arch.OUTPATH = 'D:/training/output'\n",
    "arch.WINDOW_SIZE = 250\n",
    "arch.WINDOW_SIZE_AMSR2 = 250\n",
    "arch.ASPECT_RATIO = 50\n",
    "arch.apply_instead_of_training = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9367bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extension = \".npz\"\n",
    "create_model = create_model\n",
    "input_var_names = ['nersc_sar_primary', 'nersc_sar_secondary']\n",
    "output_var_name = 'CT'\n",
    "amsr2_var_names = ['btemp_23_8h','btemp_23_8v']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dee50bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ = arch.OUTPATH\n",
    "arch.list_of_names = [join(path_, f) for f in os.listdir(path_) if (f.endswith(extension))]\n",
    "#print('bibi', arch.list_of_names)\n",
    "# appropriate folder for logging with tensorboard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b705360",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = FileBasedConfigure(arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "699d4455",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.list_of_names = [join(path_, f) for f in os.listdir(path_) if (f.endswith(extension))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f074a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.BEGINNING_OF_YEAR = 1\n",
    "conf.ENDING_DAY_OF_YEAR = 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "578666c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.filling_id_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b14490c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of training samples: 140\n",
      "total number of validation samples: 36\n"
     ]
    }
   ],
   "source": [
    "conf.divide_id_list_into_partition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22554e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.calculate_dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c710609",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.input_var_names = ['nersc_sar_primary', 'nersc_sar_secondary']\n",
    "conf.output_var_name = 'CT'\n",
    "conf.amsr2_var_names = ['btemp_23_8h','btemp_23_8v']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "153ecd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.set_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b14ca141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dims_input': (250, 250, 2),\n",
       " 'dims_output': (250, 250, 1),\n",
       " 'dims_amsr2': (5, 5, 2),\n",
       " 'output_var_name': 'CT',\n",
       " 'input_var_names': ['nersc_sar_primary', 'nersc_sar_secondary'],\n",
       " 'amsr2_var_names': ['btemp_23_8h', 'btemp_23_8v'],\n",
       " 'batch_size': 5,\n",
       " 'shuffle_on_epoch_end': True}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6152700",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.training_generator = conf.DataGenerator_(conf.partition['train'], **conf.params)\n",
    "conf.validation_generator = conf.DataGenerator_(conf.partition['validation'], **conf.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b2faf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape1 (None, 250, 250, 2)\n",
      "shape2 (None, 5, 5, 2)\n"
     ]
    }
   ],
   "source": [
    "conf.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47630c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 250, 250, 2) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 250, 250, 2)  8           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 250, 250, 8)  152         batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 125, 125, 8)  0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 125, 125, 16) 1168        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 62, 62, 16)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 62, 62, 32)   4640        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 10, 10, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 5, 5, 32)     0           average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 5, 5, 2)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 5, 5, 34)     0           average_pooling2d_3[0][0]        \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 50, 50, 34)   0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 250, 250, 34) 0           up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 250, 250, 1)  35          up_sampling2d_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 6,003\n",
      "Trainable params: 5,999\n",
      "Non-trainable params: 4\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conf.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5841850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback for tensorboard\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# callback for saving checkpoints\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=join(\"models\",datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_\"+\"{epoch:04d}\"),\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_freq=6 * conf.params[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a305fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "28/28 [==============================] - 45s 1s/step - loss: 5.6303e-06 - categorical_accuracy: 1.0000 - val_loss: 7.6516e-06 - val_categorical_accuracy: 1.0000\n",
      "Epoch 2/4\n",
      "28/28 [==============================] - ETA: 0s - loss: 5.6303e-06 - categorical_accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-22:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 726, in _run\n",
      "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 703, in pool_fn\n",
      "    pool = get_pool_class(True)(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\", line 119, in Pool\n",
      "    return Pool(processes, initializer, initargs, maxtasksperchild,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 212, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 303, in _repopulate_pool\n",
      "    return self._repopulate_pool_static(self._ctx, self.Process,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 326, in _repopulate_pool_static\n",
      "    w.start()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\process.py\", line 121, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\", line 327, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\", line 93, in __init__\n",
      "    reduction.dump(process_obj, to_child)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "RuntimeError: dictionary changed size during iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 43s 1s/step - loss: 5.6303e-06 - categorical_accuracy: 1.0000 - val_loss: 7.6516e-06 - val_categorical_accuracy: 1.0000\n",
      "Epoch 3/4\n",
      "28/28 [==============================] - ETA: 0s - loss: 5.6303e-06 - categorical_accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-35:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 726, in _run\n",
      "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 703, in pool_fn\n",
      "    pool = get_pool_class(True)(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\", line 119, in Pool\n",
      "    return Pool(processes, initializer, initargs, maxtasksperchild,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 212, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 303, in _repopulate_pool\n",
      "    return self._repopulate_pool_static(self._ctx, self.Process,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 326, in _repopulate_pool_static\n",
      "    w.start()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\process.py\", line 121, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\", line 327, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\", line 93, in __init__\n",
      "    reduction.dump(process_obj, to_child)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "RuntimeError: dictionary changed size during iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 42s 1s/step - loss: 5.6303e-06 - categorical_accuracy: 1.0000 - val_loss: 7.6516e-06 - val_categorical_accuracy: 1.0000\n",
      "Epoch 4/4\n",
      "28/28 [==============================] - ETA: 0s - loss: 5.6303e-06 - categorical_accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-47:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 726, in _run\n",
      "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
      "  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 703, in pool_fn\n",
      "    pool = get_pool_class(True)(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\", line 119, in Pool\n",
      "    return Pool(processes, initializer, initargs, maxtasksperchild,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 212, in __init__\n",
      "    self._repopulate_pool()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 303, in _repopulate_pool\n",
      "    return self._repopulate_pool_static(self._ctx, self.Process,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 326, in _repopulate_pool_static\n",
      "    w.start()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\process.py\", line 121, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\context.py\", line 327, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\", line 93, in __init__\n",
      "    reduction.dump(process_obj, to_child)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "RuntimeError: dictionary changed size during iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 41s 1s/step - loss: 5.6303e-06 - categorical_accuracy: 1.0000 - val_loss: 7.9548e-06 - val_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ac23584250>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "conf.model.fit(conf.training_generator,\n",
    "               validation_data=conf.validation_generator,\n",
    "               use_multiprocessing=True,\n",
    "               workers=4,\n",
    "               epochs=4,\n",
    "               # uncomment this line to enable the callbacks\n",
    "               #callbacks=[tensorboard_callback, cp_callback],\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abacc229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: final_model\\assets\n"
     ]
    }
   ],
   "source": [
    "conf.model.save(\"final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3016e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
