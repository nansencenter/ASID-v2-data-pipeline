{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 31.2 MB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=1.0.0\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 73.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.22.4)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1302 sha256=d193e2ebb9200b31c0ddfd2e30939ba87f1fe68c8a2bcb673aeb7742869210cf\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-w3x4aouu/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.1.1 sklearn-0.0 threadpoolctl-3.1.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-nen3egyc because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m colors\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (confusion_matrix, \n\u001b[1;32m     20\u001b[0m                             mean_squared_error, \n\u001b[1;32m     21\u001b[0m                             accuracy_score,\n\u001b[1;32m     22\u001b[0m                             precision_score,\n\u001b[1;32m     23\u001b[0m                             recall_score)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import basename, dirname, join\n",
    "\n",
    "\n",
    "import glob\n",
    "import argparse\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "\n",
    "import data_generator\n",
    "from data_generator import HugoDataGenerator, DataGenerator_sod_f, HugoBinaryGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix, \n",
    "                            mean_squared_error, \n",
    "                            accuracy_score,\n",
    "                            precision_score,\n",
    "                            recall_score)\n",
    "from scipy import stats\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense,\n",
    "                                     Flatten,\n",
    "                                     Dropout,\n",
    "                                     BatchNormalization, \n",
    "                                     Conv2D, \n",
    "                                     MaxPooling2D)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "idir = '/Data/preprocessing4hugo/output/'\n",
    "input_dir_json = '/Data/'\n",
    "\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Definition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\" Create sequential CNN with convolutional and dense layers\n",
    "    \n",
    "    \"\"\"\n",
    "    # number of ice classes\n",
    "    nbr_classes = 4  \n",
    "    # size of SAR subimages\n",
    "    ws = 50\n",
    "    # size of convolutional filters\n",
    "    cs = 3\n",
    "    # number of filters per convolutional layer (x id)\n",
    "    c1,c2,c3 = 32,32,32\n",
    "    # number of neurons per hidden neural layer number (x id)\n",
    "    n1,n2,n3 = 16,16,64\n",
    "    # value of dropout\n",
    "    dropout_rate = 0.1\n",
    "    # value of L2 regularisation\n",
    "    l2_rate = 0.001\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutional part\n",
    "    model.add(BatchNormalization(input_shape=(ws, ws, 2)))\n",
    "    model.add(Conv2D(c1, (cs, cs), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2),2))\n",
    "    model.add(Conv2D(c2, (cs, cs), activation='relu'))\n",
    "    model.add(Conv2D(c3, (cs, cs), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2),2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Neural network part (hidden layers)\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(n1, kernel_regularizer=l2(l2_rate), activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(n2, kernel_regularizer=l2(l2_rate), activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(n3, kernel_regularizer=l2(l2_rate), activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Last neural layer (not hidden)\n",
    "    model.add(Dense(nbr_classes, kernel_regularizer=l2(l2_rate), activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Parameters and load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "with open(f'{idir}processed_files.json') as fichier_json:\n",
    "    all_nc = json.load(fichier_json)\n",
    "npz_files=[]\n",
    "\n",
    "for nc in all_nc :\n",
    "    name = nc[:15]\n",
    "    files = sorted(glob.glob(f'{idir}/{name}/*.npz'))\n",
    "    npz_files += files\n",
    "random.shuffle(npz_files)\n",
    "\n",
    "npz_files = npz_files[:10000]\n",
    "\n",
    "print('Files number : '+ str (len(npz_files)))\n",
    "train_files_number = int(len(npz_files) * train_ratio)\n",
    "train_files = npz_files[:train_files_number]\n",
    "valid_files = npz_files[train_files_number:]\n",
    "\n",
    "input_var_names = ['nersc_sar_primary', 'nersc_sar_secondary']\n",
    "amsr2_var_names = [ 'btemp_6_9h',\n",
    "                    'btemp_6_9v',\n",
    "                    'btemp_7_3h',\n",
    "                    'btemp_7_3v',\n",
    "                    'btemp_10_7h',\n",
    "                    'btemp_10_7v',\n",
    "                    'btemp_18_7h',\n",
    "                    'btemp_18_7v',\n",
    "                    'btemp_23_8h',\n",
    "                    'btemp_23_8v',\n",
    "                    'btemp_36_5h',\n",
    "                    'btemp_36_5v',\n",
    "                    'btemp_89_0h',\n",
    "                    'btemp_89_0v'\n",
    "                  ]\n",
    " \n",
    "output_var_name = 'ice_type'\n",
    "dims_amsr2 = np.load(npz_files[0])[amsr2_var_names[0]].shape\n",
    "\n",
    "params = {'dims_amsr2':      (*dims_amsr2, len(amsr2_var_names)),\n",
    "          'idir_json':       input_dir_json,\n",
    "          'output_var_name': output_var_name,\n",
    "          'input_var_names': input_var_names,\n",
    "          'amsr2_var_names': amsr2_var_names,\n",
    "          'batch_size':      50,\n",
    "          'shuffle_on_epoch_end': False,\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 processed data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_generator = HugoBinaryGenerator(train_files, **params)\n",
    "validation_generator = HugoBinaryGenerator(valid_files, **params)\n",
    "\n",
    "# training_generator = HugoDataGenerator(train_files, **params)\n",
    "# validation_generator = HugoDataGenerator(valid_files, **params)\n",
    "\n",
    "# training_generator = DataGenerator_sod_f(train_files, **params)\n",
    "# validation_generator = DataGenerator_sod_f(valid_files, **params)\n",
    "\n",
    "print(\n",
    "     training_generator[0][0].shape, #sar\n",
    "#     training_generator[0][0][1].shape, #amsr2\n",
    "     training_generator[0][1].shape, #output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load model and trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load model and trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of the model \n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Apply CNN to SAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#callbacks\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(filepath='hugo_model_1000', \n",
    "                                        monitor='val_loss',\n",
    "                                        verbose=1, \n",
    "                                        save_best_only=True,\n",
    "                                        mode='min')\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "#optimizers and compile\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy')\n",
    "\n",
    "#fit\n",
    "model.summary()\n",
    "history = model.fit(training_generator, \n",
    "                    use_multiprocessing=True,\n",
    "                    workers=4,\n",
    "                    validation_data=validation_generator,\n",
    "                    epochs=20, \n",
    "                    callbacks=[mc, es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 History of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(validation_generator)\n",
    "y_val = np.vstack([vg[1] for vg in validation_generator])\n",
    "y_val_index =[]\n",
    "y_pred_index =[]\n",
    "for val, pred in zip(y_val, y_pred):\n",
    "    if max(pred)>0.5 :\n",
    "        y_val_index.append(np.argmax(val))\n",
    "        y_pred_index.append(np.argmax(pred))\n",
    "    \n",
    "cm = confusion_matrix(y_val_index, y_pred_index)\n",
    "print(cm)\n",
    "plt.clf()\n",
    "plt.imshow(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analysis confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AC = accuracy_score(y_val_index, y_pred_index)\n",
    "print ('Accuracy : ' +str(AC))\n",
    "\n",
    "P_macro = precision_score(y_val_index, y_pred_index, average='macro')\n",
    "P_micro = precision_score(y_val_index, y_pred_index, average='micro')\n",
    "print('Precision : ' + str(P_macro)+ \" \" +str(P_micro))\n",
    "\n",
    "R_macro = recall_score(y_val_index, y_pred_index, average='macro') \n",
    "R_micro = recall_score(y_val_index, y_pred_index, average='micro') \n",
    "print('Recall : ' + str(R_macro)+ \" \" +str(R_micro))\n",
    "\n",
    "pearson_correlation = stats.pearsonr(y_val_index, y_pred_index)\n",
    "print(pearson_correlation[0])\n",
    "\n",
    "rmse = mean_squared_error(y_val_index, y_pred_index)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_ in range (cm.shape[0]) :\n",
    "    print('class : '+ str(class_))\n",
    "    TP = cm[class_][class_]\n",
    "    print ('TP : '+ str(TP))\n",
    "    \n",
    "    M = np.delete(cm, (class_), axis=0)\n",
    "    M = np.delete(M, (class_), axis=1)\n",
    "    \n",
    "    TN =np.sum(M)\n",
    "    print ('TN : '+ str(TN))\n",
    "    \n",
    "    \n",
    "    lign = cm[0,1:]\n",
    "    #recuperer la ligne puis supprimer l'element a l'indice class_\n",
    "#     FP = \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_val[:,0], y_pred[:,0], '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(cm)\n",
    "# print (cm[1:,1:])\n",
    "print(cm[0,1:])\n",
    "print(cm[1:,0])\n",
    "\n",
    "# M = np.delete(cm, (2), axis=0)\n",
    "# M = np.delete(M, (2), axis=1)\n",
    "# print (M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "modelbis = keras.models.load_model('/tf/sea_ice_type_cnn_training/asip_v2/hugo_model_test_all/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = modelbis.predict(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
