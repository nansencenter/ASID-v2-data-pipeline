{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /.local/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /.local/lib/python3.8/site-packages (from sklearn) (1.1.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.22.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in /.local/lib/python3.8/site-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from seaborn) (1.22.4)\n",
      "Requirement already satisfied: pandas>=0.23 in /.local/lib/python3.8/site-packages (from seaborn) (1.4.3)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.8/dist-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.8/dist-packages (from seaborn) (1.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.23->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /.local/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2022.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (4.33.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (9.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas>=0.23->seaborn) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-xxm0ra21 because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import basename, dirname, join\n",
    "\n",
    "\n",
    "import glob\n",
    "import argparse\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sebrn\n",
    "import data_generator\n",
    "from data_generator import (HugoDataGenerator,\n",
    "                            DataGenerator_sod_f_all,\n",
    "                            DataGenerator_sod_f, \n",
    "                            HugoBinaryGenerator, \n",
    "                            HugoSarDataGenerator, \n",
    "                            HugoAMRS2DataGenerator)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix, \n",
    "                            mean_squared_error, \n",
    "                            accuracy_score,\n",
    "                            precision_score,\n",
    "                            recall_score)\n",
    "from scipy import stats\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense,\n",
    "                                     Flatten,\n",
    "                                     Dropout,\n",
    "                                     BatchNormalization, \n",
    "                                     Conv2D, \n",
    "                                     MaxPooling2D,\n",
    "                                     Concatenate)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "input_dir_json = '/tf/data/hugo_sod/'\n",
    "idir = '/tf/data/hugo_sod/output_preprocessed/'\n",
    "\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Definition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    create a keras model \n",
    "    \"\"\"\n",
    "        # number of ice classes\n",
    "    nbr_classes = 32  \n",
    "    # size of SAR subimages\n",
    "    ws = 50\n",
    "    # size of AMRS2 subimages\n",
    "    ws2 = 10\n",
    "    # size of convolutional filters\n",
    "    cs = 3\n",
    "    # number of filters per convolutional layer (x id)\n",
    "    c1,c2,c3 = 32,32,32\n",
    "    # number of neurons per hidden neural layer number (x id)\n",
    "    n1,n2,n3 = 16,16,64\n",
    "    # value of dropout\n",
    "    dropout_rate = 0.1\n",
    "    # value of L2 regularisation\n",
    "    l2_rate = 0.001\n",
    "    \n",
    "    \n",
    "    input_ = layers.Input(shape =(ws, ws, 4) )\n",
    "    input_2 = layers.Input(shape = (ws2, ws2, 14))\n",
    "    \n",
    "    x = layers.BatchNormalization()(input_)\n",
    "    x = layers.Conv2D(c1, (cs, cs),  activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2), strides=2)(x)\n",
    "    x = layers.Conv2D(c2, (cs, cs), activation='relu')(x)\n",
    "    x = layers.Conv2D(c3, (cs, cs), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2), strides=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    input_2 = BatchNormalization()(input_2)\n",
    "\n",
    "    x = layers.Concatenate()([x, input_2])\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(n1, kernel_regularizer=l2(l2_rate), activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(n2, kernel_regularizer=l2(l2_rate), activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(n3, kernel_regularizer=l2(l2_rate), activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    # Last neural layer (not hidden)\n",
    "    x = layers.Dense(nbr_classes, kernel_regularizer=l2(l2_rate), activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[input_, input_2], outputs=x)\n",
    "    opt = tf.keras.optimizers.Adam()\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Parameters and load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files number : 888522\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.7\n",
    "with open(f'{idir}processed_files.json') as fichier_json:\n",
    "    all_nc = json.load(fichier_json)\n",
    "npz_files=[]\n",
    "# all_nc=[\"20180410T084537_S1B_AMSR2_Icechart-Greenland-SouthEast.nc\"]\n",
    "\n",
    "for nc in all_nc :\n",
    "    name = nc[:15]\n",
    "    files = sorted(glob.glob(f'{idir}/{name}/*.npz'))\n",
    "    npz_files += files\n",
    "random.shuffle(npz_files)\n",
    "\n",
    "# npz_files = npz_files[:10000]\n",
    "\n",
    "print('Files number : '+ str (len(npz_files)))\n",
    "train_files_number = int(len(npz_files) * train_ratio)\n",
    "train_files = npz_files[:train_files_number]\n",
    "valid_files = npz_files[train_files_number:]\n",
    "\n",
    "input_var_names = ['nersc_sar_primary', 'nersc_sar_secondary']\n",
    "amsr2_var_names = [\n",
    "    'btemp_6_9h',\n",
    "                   'btemp_6_9v',\n",
    "                   'btemp_7_3h',\n",
    "                   'btemp_7_3v',\n",
    "                   'btemp_10_7h',\n",
    "                   'btemp_10_7v',\n",
    "                   'btemp_18_7h',\n",
    "                   'btemp_18_7v',\n",
    "                   'btemp_23_8h',\n",
    "                   'btemp_23_8v',\n",
    "                   'btemp_36_5h',\n",
    "                   'btemp_36_5v',\n",
    "                   'btemp_89_0h',\n",
    "                   'btemp_89_0v'\n",
    "                  ]\n",
    "\n",
    "output_var_name = 'ice_type'\n",
    "dims_amsr2 = np.load(npz_files[0])[amsr2_var_names[0]].shape\n",
    "\n",
    "\n",
    "params = {'dims_amsr2':      (*dims_amsr2, len(amsr2_var_names)),\n",
    "          'idir_json':       input_dir_json,\n",
    "          'output_var_name': output_var_name,\n",
    "          'input_var_names': input_var_names,\n",
    "          'amsr2_var_names': amsr2_var_names,\n",
    "          'batch_size':      50,\n",
    "          'shuffle_on_epoch_end': False,\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 processed data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 32)\n"
     ]
    }
   ],
   "source": [
    "training_generator = DataGenerator_sod_f_all(train_files, **params)\n",
    "validation_generator = DataGenerator_sod_f_all(valid_files, **params)\n",
    "\n",
    "print(\n",
    "#      training_generator[0][0][0].shape, #sar\n",
    "#      training_generator[0][0][1].shape, #amsr2\n",
    "     training_generator[0][1].shape, #output\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load model and trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load model and trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of the model \n",
    "model = create_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Apply CNN to SAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 50, 50, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 50, 50, 4)   16          ['input_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 48, 48, 32)   1184        ['batch_normalization[1][0]']    \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 24, 24, 32)   0           ['conv2d[1][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 22, 22, 32)   9248        ['max_pooling2d[1][0]']          \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 20, 20, 32)   9248        ['conv2d_1[1][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 10, 10, 32)  0           ['conv2d_2[1][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 10, 10, 32)  128         ['max_pooling2d_1[1][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 10, 10, 14)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 10, 10, 46)   0           ['batch_normalization_1[1][0]',  \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 4600)         0           ['concatenate[1][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4600)         0           ['flatten[1][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           73616       ['dropout[1][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 16)           0           ['dense[1][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 16)           272         ['dropout_1[1][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 16)           0           ['dense_1[1][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           1088        ['dropout_2[1][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 64)           0           ['dense_2[1][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 32)           2080        ['dropout_3[1][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 96,880\n",
      "Trainable params: 96,808\n",
      "Non-trainable params: 72\n",
      "__________________________________________________________________________________________________\n",
      "   56/12439 [..............................] - ETA: 15:43 - loss: 0.0843"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Keras_worker_ForkPoolWorker-3:\n",
      "Process Keras_worker_ForkPoolWorker-2:\n",
      "Process Keras_worker_ForkPoolWorker-4:\n",
      "Process Keras_worker_ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 580, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 580, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 580, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/tf/sea_ice_type_cnn_training/asip_v2/data_generator.py\", line 369, in __getitem__\n",
      "    self.data_generation()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/data_utils.py\", line 580, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/tf/sea_ice_type_cnn_training/asip_v2/data_generator.py\", line 369, in __getitem__\n",
      "    self.data_generation()\n",
      "  File \"/tf/sea_ice_type_cnn_training/asip_v2/data_generator.py\", line 369, in __getitem__\n",
      "    self.data_generation()\n",
      "  File \"/tf/sea_ice_type_cnn_training/asip_v2/data_generator.py\", line 388, in data_generation\n",
      "    batch.update(np.load(ID))\n",
      "  File \"/tf/sea_ice_type_cnn_training/asip_v2/data_generator.py\", line 369, in __getitem__\n",
      "    self.data_generation()\n",
      "  File \"/tf/sea_ice_type_cnn_training/asip_v2/data_generator.py\", line 388, in data_generation\n",
      "    batch.update(np.load(ID))\n",
      "  File \"/tf/sea_ice_type_cnn_training/asip_v2/data_generator.py\", line 388, in data_generation\n",
      "    batch.update(np.load(ID))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py\", line 243, in __getitem__\n",
      "    return format.read_array(bytes,\n",
      "  File \"/tf/sea_ice_type_cnn_training/asip_v2/data_generator.py\", line 388, in data_generation\n",
      "    batch.update(np.load(ID))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py\", line 243, in __getitem__\n",
      "    return format.read_array(bytes,\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py\", line 243, in __getitem__\n",
      "    return format.read_array(bytes,\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/numpy/lib/format.py\", line 732, in read_array\n",
      "    shape, fortran_order, dtype = _read_array_header(fp, version)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/numpy/lib/format.py\", line 779, in read_array\n",
      "    array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py\", line 243, in __getitem__\n",
      "    return format.read_array(bytes,\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/numpy/lib/format.py\", line 732, in read_array\n",
      "    shape, fortran_order, dtype = _read_array_header(fp, version)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/numpy/lib/format.py\", line 597, in _read_array_header\n",
      "    d = safe_eval(header)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/numpy/lib/format.py\", line 595, in _read_array_header\n",
      "    header = _filter_header(header)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/numpy/lib/format.py\", line 778, in read_array\n",
      "    data = _read_bytes(fp, read_size, \"array data\")\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/numpy/lib/utils.py\", line 1002, in safe_eval\n",
      "    return ast.literal_eval(source)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/numpy/lib/format.py\", line 556, in _filter_header\n",
      "    for token in tokenize.generate_tokens(StringIO(s).readline):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/numpy/lib/format.py\", line 907, in _read_bytes\n",
      "    r = fp.read(size - len(data))\n",
      "  File \"/usr/lib/python3.8/ast.py\", line 59, in literal_eval\n",
      "    node_or_string = parse(node_or_string, mode='eval')\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/zipfile.py\", line 940, in read\n",
      "    data = self._read1(n)\n",
      "  File \"/usr/lib/python3.8/ast.py\", line 47, in parse\n",
      "    return compile(source, filename, mode, flags,\n",
      "  File \"/usr/lib/python3.8/zipfile.py\", line 1030, in _read1\n",
      "    self._update_crc(data)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/zipfile.py\", line 955, in _update_crc\n",
      "    self._running_crc = crc32(newdata, self._running_crc)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#callbacks\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(filepath='hugo_model_1000', \n",
    "                                        monitor='val_loss',\n",
    "                                        verbose=1, \n",
    "                                        save_best_only=True,\n",
    "                                        mode='min')\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "history = model.fit(training_generator, \n",
    "                    use_multiprocessing=True,\n",
    "                    workers=4,\n",
    "                    validation_data=validation_generator,\n",
    "                    epochs=1, \n",
    "                    callbacks=[mc, es])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 History of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(validation_generator)\n",
    "y_val = np.vstack([vg[1] for vg in validation_generator])\n",
    "\n",
    "\n",
    "y_val_index =[]\n",
    "y_pred_index =[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson ,rmse, rrmse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rmse_matrix=np.empty((32,32))\n",
    "pearson_matrix = np.empty((32,32))\n",
    "for id_class_pred in range(y_pred.shape[1]):\n",
    "    classes_pred = y_pred[:,id_class_pred]\n",
    "    for id_class_val in range (y_val.shape[1]):\n",
    "        classes_val = y_val[:,id_class_val]\n",
    "        rmse = mean_squared_error(classes_val, classes_pred)\n",
    "        rmse_matrix[id_class_pred][id_class_val] = rmse\n",
    "        pearson_value = stats.pearsonr(classes_val, classes_pred)\n",
    "        pearson_matrix[id_class_pred][id_class_val] = pearson_value[0]\n",
    "\n",
    "print('rmse_matrix')\n",
    "rm=[]\n",
    "for i in rmse_matrix :\n",
    "    l=[]\n",
    "    for j in i :\n",
    "        l.append(np.round(j,8))\n",
    "    rm.append(l)\n",
    "print(rm)\n",
    "\n",
    "    \n",
    "print('pearson_matrix')\n",
    "pm=[]\n",
    "for i in pearson_matrix :\n",
    "    l=[]\n",
    "    for j in i :\n",
    "        l.append(np.round(j,8))\n",
    "    pm.append(l)\n",
    "print(pm)\n",
    "\n",
    "\n",
    "with open(f'{input_dir_json}/vector_combinations.json') as fichier_json:\n",
    "    list_combi = json.load(fichier_json)['all_work_comb']\n",
    "y_ticks= [(i+0.5) for i in range (32)]\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(18,18))\n",
    "fx = sebrn.heatmap(pearson_matrix, annot=True, cmap='bwr', fmt=\".3f\", cbar=False)\n",
    "fx.set_title('Pearson correlation Matrix \\n')\n",
    "fx.set_xlabel('Predicted Values')\n",
    "fx.set_ylabel('True Values ')\n",
    "fx.xaxis.set_ticklabels(list_combi)\n",
    "fx.xaxis.tick_top()\n",
    "fx.set_yticks(y_ticks)\n",
    "fx.yaxis.set_ticklabels(ticklabels=list_combi)\n",
    "plt.savefig('test_pm')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Using Seaborn heatmap to create the plot\n",
    "plt.clf()\n",
    "plt.figure(figsize=(18,18))\n",
    "fx = sebrn.heatmap(rmse_matrix, annot=True, cmap='Blues', fmt=\".3f\", cbar=False)\n",
    "fx.set_title('RMSE Matrix \\n')\n",
    "fx.set_xlabel('Predicted Values')\n",
    "fx.set_ylabel('True Values ')\n",
    "fx.xaxis.set_ticklabels(list_combi)\n",
    "fx.xaxis.tick_top()\n",
    "fx.set_yticks(y_ticks)\n",
    "fx.yaxis.set_ticklabels(ticklabels=list_combi)\n",
    "plt.savefig('test_rmse')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
