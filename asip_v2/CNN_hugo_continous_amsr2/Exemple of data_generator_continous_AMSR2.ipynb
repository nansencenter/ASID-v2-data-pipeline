{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /.local/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /.local/lib/python3.8/site-packages (from sklearn) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.22.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-d1qqwamc because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import basename, dirname, join\n",
    "\n",
    "\n",
    "import glob\n",
    "import argparse\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "\n",
    "import data_generator\n",
    "from data_generator import (HugoDataGenerator,\n",
    "                            DataGenerator_sod_f, \n",
    "                            HugoBinaryGenerator, \n",
    "                            HugoSarDataGenerator, \n",
    "                            HugoAMRS2DataGenerator)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix, \n",
    "                            mean_squared_error, \n",
    "                            accuracy_score,\n",
    "                            precision_score,\n",
    "                            recall_score)\n",
    "from scipy import stats\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense,\n",
    "                                     Flatten,\n",
    "                                     Dropout,\n",
    "                                     BatchNormalization, \n",
    "                                     Conv2D, \n",
    "                                     MaxPooling2D,\n",
    "                                     Concatenate)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# # idir = '/Data/preprocessing4hugo/output/'\n",
    "# input_dir_json = '/Data/tmp/'\n",
    "# idir = '/Data/tmp/output/'\n",
    "# input_dir_json = '/tf/data/hugo_continous_amsr2/output_preprocessed_continous/check/'\n",
    "input_dir_json = '/tf/data/hugo_continous_amsr2/output_preprocessed_continous/'\n",
    "idir = '/tf/data/hugo_continous_amsr2/output_preprocessed_continous/output/'\n",
    "\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Definition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    create a keras model \n",
    "    \"\"\"\n",
    "        # number of ice classes\n",
    "    nbr_classes = 4  \n",
    "    # size of SAR subimages\n",
    "    ws = 50\n",
    "    # size of AMRS2 subimages\n",
    "    ws2 = 10\n",
    "    # size of convolutional filters\n",
    "    cs = 3\n",
    "    # number of filters per convolutional layer (x id)\n",
    "    c1,c2,c3 = 32,32,32\n",
    "    # number of neurons per hidden neural layer number (x id)\n",
    "    n1,n2,n3 = 16,16,64\n",
    "    # value of dropout\n",
    "    dropout_rate = 0.1\n",
    "    # value of L2 regularisation\n",
    "    l2_rate = 0.001\n",
    "    \n",
    "    model\n",
    "    \n",
    "    input_ = layers.Input(shape =(ws, ws, 4) )\n",
    "    input_2 = layers.Input(shape = (ws2, ws2, 6))\n",
    "    \n",
    "    x = layers.BatchNormalization()(input_)\n",
    "    x = layers.Conv2D(c1, (cs, cs),  activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2), strides=2)(x)\n",
    "    x = layers.Conv2D(c2, (cs, cs), activation='relu')(x)\n",
    "    x = layers.Conv2D(c3, (cs, cs), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2), strides=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    input_2 = BatchNormalization()(input_2)\n",
    "\n",
    "    x = layers.Concatenate()([x, input_2])\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(n1, kernel_regularizer=l2(l2_rate), activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(n2, kernel_regularizer=l2(l2_rate), activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(n3, kernel_regularizer=l2(l2_rate), activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    # Last neural layer (not hidden)\n",
    "    x = layers.Dense(nbr_classes, kernel_regularizer=l2(l2_rate), activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[input_, input_2], outputs=x)\n",
    "    opt = tf.keras.optimizers.Adam()\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Parameters and load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files number : 10000\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.7\n",
    "with open(f'{idir}processed_files.json') as fichier_json:\n",
    "    all_nc = json.load(fichier_json)\n",
    "npz_files=[]\n",
    "\n",
    "\n",
    "for nc in all_nc :\n",
    "    name = nc[:15]\n",
    "    files = sorted(glob.glob(f'{idir}/{name}/*.npz'))\n",
    "    npz_files += files\n",
    "random.shuffle(npz_files)\n",
    "\n",
    "npz_files = npz_files[:10000]\n",
    "\n",
    "print('Files number : '+ str (len(npz_files)))\n",
    "train_files_number = int(len(npz_files) * train_ratio)\n",
    "train_files = npz_files[:train_files_number]\n",
    "valid_files = npz_files[train_files_number:]\n",
    "\n",
    "input_var_names = ['nersc_sar_primary', 'nersc_sar_secondary']\n",
    "amsr2_var_names = [\n",
    "#     'btemp_6_9h',\n",
    "#                    'btemp_6_9v',\n",
    "#                    'btemp_7_3h',\n",
    "#                    'btemp_7_3v',\n",
    "#                    'btemp_10_7h',\n",
    "#                    'btemp_10_7v',\n",
    "#                    'btemp_18_7h',\n",
    "#                    'btemp_18_7v',\n",
    "                   'btemp_23_8h',\n",
    "                   'btemp_23_8v',\n",
    "                   'btemp_36_5h',\n",
    "                   'btemp_36_5v',\n",
    "                   'btemp_89_0h',\n",
    "                   'btemp_89_0v'\n",
    "                  ]\n",
    "\n",
    "output_var_name = 'ice_type'\n",
    "dims_amsr2 = np.load(npz_files[0])[amsr2_var_names[0]].shape\n",
    "\n",
    "\n",
    "params = {'dims_amsr2':      (*dims_amsr2, len(amsr2_var_names)),\n",
    "          'idir_json':       input_dir_json,\n",
    "          'output_var_name': output_var_name,\n",
    "          'input_var_names': input_var_names,\n",
    "          'amsr2_var_names': amsr2_var_names,\n",
    "          'batch_size':      50,\n",
    "          'shuffle_on_epoch_end': False,\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 processed data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 4)\n"
     ]
    }
   ],
   "source": [
    "training_generator = HugoAMRS2DataGenerator(train_files, **params)\n",
    "validation_generator = HugoAMRS2DataGenerator(valid_files, **params)\n",
    "\n",
    "print(\n",
    "#      training_generator[0][0][0].shape, #sar\n",
    "#      training_generator[0][0][1].shape, #amsr2\n",
    "     training_generator[0][1].shape, #output\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load model and trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load model and trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of the model \n",
    "model = create_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Apply CNN to SAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#callbacks\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(filepath='hugo_model_1000', \n",
    "                                        monitor='val_loss',\n",
    "                                        verbose=1, \n",
    "                                        save_best_only=True,\n",
    "                                        mode='min')\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "history = model.fit(training_generator, \n",
    "                    use_multiprocessing=True,\n",
    "                    workers=4,\n",
    "                    validation_data=validation_generator,\n",
    "                    epochs=20, \n",
    "                    callbacks=[mc, es])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 History of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(validation_generator)\n",
    "y_val = np.vstack([vg[1] for vg in validation_generator])\n",
    "\n",
    "y_val_index =[]\n",
    "y_pred_index =[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson and rmse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_matrix=np.empty((4,4))\n",
    "pearson_matrix = np.empty((4,4))\n",
    "for id_class_pred in range(y_pred.shape[1]):\n",
    "    classes_pred = y_pred[:,id_class_pred]\n",
    "    for id_class_val in range (y_val.shape[1]):\n",
    "        classes_val = y_val[:,id_class_val]\n",
    "        rmse = mean_squared_error(classes_val, classes_pred)\n",
    "        rmse_matrix[id_class_pred][id_class_val] = rmse\n",
    "        pearson_value = stats.pearsonr(classes_val, classes_pred)\n",
    "        pearson_matrix[id_class_pred][id_class_val] = pearson_value[0]\n",
    "\n",
    "print('rmse_matrix')\n",
    "print(rmse_matrix)\n",
    "print('pearson_matrix')\n",
    "print(pearson_matrix)\n",
    "\n",
    "plt.imshow(rmse_matrix)\n",
    "plt.show()\n",
    "plt.imshow(pearson_matrix)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
